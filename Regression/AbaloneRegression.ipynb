{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 预测鲍鱼年龄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"\n",
    "    函数说明:加载数据\n",
    "    Parameters:\n",
    "        fileName - 文件名\n",
    "    Returns:\n",
    "        xArr - x数据集\n",
    "        yArr - y数据集\n",
    "    Website:\n",
    "https://www.cuijiahua.com/\n",
    "    Modify:\n",
    "        2017-11-19\n",
    "    \"\"\"\n",
    "    numFeat = len(open(fileName).readline().split('\\t')) - 1\n",
    "    xArr = []; yArr = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr =[]\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        xArr.append(lineArr)\n",
    "        yArr.append(float(curLine[-1]))\n",
    "    return xArr, yArr\n",
    "def lwlr(testPoint, xArr, yArr, k = 1.0):\n",
    "    \"\"\"\n",
    "    函数说明:使用局部加权线性回归计算回归系数w\n",
    "    Parameters:\n",
    "        testPoint - 测试样本点\n",
    "        xArr - x数据集\n",
    "        yArr - y数据集\n",
    "        k - 高斯核的k,自定义参数\n",
    "    Returns:\n",
    "        ws - 回归系数\n",
    "    Website:\n",
    "https://www.cuijiahua.com/\n",
    "    Modify:\n",
    "        2017-11-19\n",
    "    \"\"\"\n",
    "    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n",
    "    m = np.shape(xMat)[0]\n",
    "    weights = np.mat(np.eye((m)))                                        #创建权重对角矩阵\n",
    "    for j in range(m):                                                  #遍历数据集计算每个样本的权重\n",
    "        diffMat = testPoint - xMat[j, :]                                 \n",
    "        weights[j, j] = np.exp(diffMat * diffMat.T/(-2.0 * k**2))\n",
    "    xTx = xMat.T * (weights * xMat)                                        \n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"矩阵为奇异矩阵,不能求逆\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T * (weights * yMat))                            #计算回归系数\n",
    "    return testPoint * ws\n",
    "def lwlrTest(testArr, xArr, yArr, k=1.0):  \n",
    "    \"\"\"\n",
    "    函数说明:局部加权线性回归测试\n",
    "    Parameters:\n",
    "        testArr - 测试数据集,测试集\n",
    "        xArr - x数据集,训练集\n",
    "        yArr - y数据集,训练集\n",
    "        k - 高斯核的k,自定义参数\n",
    "    Returns:\n",
    "        ws - 回归系数\n",
    "    Website:\n",
    "https://www.cuijiahua.com/\n",
    "    Modify:\n",
    "        2017-11-19\n",
    "    \"\"\"\n",
    "    m = np.shape(testArr)[0]                                            #计算测试数据集大小\n",
    "    yHat = np.zeros(m)    \n",
    "    for i in range(m):                                                    #对每个样本点进行预测\n",
    "        yHat[i] = lwlr(testArr[i],xArr,yArr,k)\n",
    "    return yHat\n",
    "def standRegres(xArr,yArr):\n",
    "    \"\"\"\n",
    "    函数说明:计算回归系数w\n",
    "    Parameters:\n",
    "        xArr - x数据集\n",
    "        yArr - y数据集\n",
    "    Returns:\n",
    "        ws - 回归系数\n",
    "    Website:\n",
    "https://www.cuijiahua.com/\n",
    "    Modify:\n",
    "        2017-11-19\n",
    "    \"\"\"\n",
    "    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n",
    "    xTx = xMat.T * xMat                            #根据文中推导的公示计算回归系数\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"矩阵为奇异矩阵,不能求逆\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T*yMat)\n",
    "    return ws\n",
    "def rssError(yArr, yHatArr):\n",
    "    \"\"\"\n",
    "    误差大小评价函数\n",
    "    Parameters:\n",
    "        yArr - 真实数据\n",
    "        yHatArr - 预测数据\n",
    "    Returns:\n",
    "        误差大小\n",
    "    \"\"\"\n",
    "    return ((yArr - yHatArr) **2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集与测试集相同:局部加权线性回归,核k的大小对预测的影响:\n",
      "k=0.1时,误差大小为: 56.78868743050092\n",
      "k=1  时,误差大小为: 429.89056187038\n",
      "k=10 时,误差大小为: 549.1181708827924\n",
      "\n",
      "训练集与测试集不同:局部加权线性回归,核k的大小是越小越好吗？更换数据集,测试结果如下:\n",
      "k=0.1时,误差大小为: 57913.51550155911\n",
      "k=1  时,误差大小为: 573.5261441895982\n",
      "k=10 时,误差大小为: 517.5711905381903\n",
      "\n",
      "训练集与测试集不同:简单的线性归回与k=1时的局部加权线性回归对比:\n",
      "k=1时,误差大小为: 573.5261441895982\n",
      "简单的线性回归误差大小: 518.6363153245542\n"
     ]
    }
   ],
   "source": [
    "abX, abY = loadDataSet('abalone.txt')\n",
    "print('训练集与测试集相同:局部加权线性回归,核k的大小对预测的影响:')\n",
    "yHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)\n",
    "yHat1 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1)\n",
    "yHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10)\n",
    "print('k=0.1时,误差大小为:',rssError(abY[0:99], yHat01.T))\n",
    "print('k=1  时,误差大小为:',rssError(abY[0:99], yHat1.T))\n",
    "print('k=10 时,误差大小为:',rssError(abY[0:99], yHat10.T))\n",
    "print('')\n",
    "print('训练集与测试集不同:局部加权线性回归,核k的大小是越小越好吗？更换数据集,测试结果如下:')\n",
    "yHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)\n",
    "yHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)\n",
    "yHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)\n",
    "print('k=0.1时,误差大小为:',rssError(abY[100:199], yHat01.T))\n",
    "print('k=1  时,误差大小为:',rssError(abY[100:199], yHat1.T))\n",
    "print('k=10 时,误差大小为:',rssError(abY[100:199], yHat10.T))\n",
    "print('')\n",
    "print('训练集与测试集不同:简单的线性归回与k=1时的局部加权线性回归对比:')\n",
    "print('k=1时,误差大小为:', rssError(abY[100:199], yHat1.T))\n",
    "ws = standRegres(abX[0:99], abY[0:99])\n",
    "yHat = np.mat(abX[100:199]) * ws\n",
    "print('简单的线性回归误差大小:', rssError(abY[100:199], yHat.T.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，当k=0.1时，训练集误差小，但是应用于新的数据集之后，误差反而变大了。这就是经常说道的过拟合现象。我们训练的模型，我们要保证测试集准确率高，这样训练出的模型才可以应用于新的数据，也就是要加强模型的普适性。可以看到，当k=1时，局部加权线性回归和简单的线性回归得到的效果差不多。这也表明一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是10吗？或许是，但如果想得到更好的效果，应该用10个不同的样本集做10次测试来比较结果。\n",
    "\n",
    "本示例展示了如何使用局部加权线性回归来构建模型，可以得到比普通线性回归更好的效果。局部加权线性回归的问题在于，每次必须在整个数据集上运行。也就是说为了做出预测，必须保存所有的训练数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "在局部加权线性回归中，过小的核可能导致过拟合现象，即训练集表现良好，测试集表现就渣渣了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
